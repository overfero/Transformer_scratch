{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58bfcbac-e0d0-4e70-9d6c-2a02d1a70243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b38bd376-1282-4e1f-bead-23d6c8263c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6e682425-2208-4328-906f-f64dc7bc37e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncodings(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0)/d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62aa1d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(1))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim=True)\n",
    "        std = x.std(dim = -1, keepdim=True)\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "faeffbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_2(self.dropout(nn.ReLU(self.linear_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "83e9459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, h: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.h = h\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        assert d_model % h == 0, \"d_model must be divisible by h\"\n",
    "\n",
    "        self.d_k = d_model // h\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    @staticmethod\n",
    "    def attention(q, k, v, mask, dropout: nn.Dropout):\n",
    "        d_k = q.shape[-1]\n",
    "\n",
    "        att_score = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            att_score = att_score.masked_fill_(mask == 0, -1e-9)\n",
    "        att_score = att_score.softmax(dim = -1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            att_score = dropout(att_score)\n",
    "        return att_score.matmul(v), att_score\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q)\n",
    "        key = self.w_k(k)\n",
    "        value = self.w_v(v)\n",
    "\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        x, att_score = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        return self.w_o(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08cddbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, dropout: float):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f46e374",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, self_att_block: MultiHeadAttentionBlock, feed_frwd_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_att_block = self_att_block\n",
    "        self.feed_frwd_block = feed_frwd_block\n",
    "        self.residual_connection = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.residual_connection[0](x, lambda x: self.self_att_block(x, x, x, src_mask))\n",
    "        x = self.residual_connection[1](x, self.feed_frwd_block)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4de535c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "28ee67ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 self_att_block: MultiHeadAttentionBlock, \n",
    "                 cross_att_block: MultiHeadAttentionBlock, \n",
    "                 feed_frwd_block: FeedForwardBlock, \n",
    "                 dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_att_block = self_att_block\n",
    "        self.cross_att_block = cross_att_block\n",
    "        self.feed_frwd_block = feed_frwd_block\n",
    "        self.residual_connection = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        x = self.residual_connection[0](x, lambda x: self.self_att_block(x, x, x, tgt_mask))\n",
    "        x = self.residual_connection[1](x, lambda x: self.cross_att_block(x, enc_output, enc_output, src_mask))\n",
    "        x = self.residual_connection[2](x, self.feed_frwd_block)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ee5ad952",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization()\n",
    "    \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "962c4016",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.log_softmax(self.proj(x), dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "772371b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            encoder: Encoder, \n",
    "            decoder: Decoder, \n",
    "            src_embed: InputEmbeddings, \n",
    "            tgt_embed: InputEmbeddings, \n",
    "            src_pos: PositionalEncodings, \n",
    "            tgt_pos: PositionalEncodings,\n",
    "            proj_layer: ProjectionLayer) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.proj_layer = proj_layer\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "\n",
    "    def decode(self, enc_output, src_mask, tgt, tgt_mask):\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, enc_output, src_mask, tgt_mask)\n",
    "    \n",
    "    def project(self, x):\n",
    "        return self.proj_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "958f513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(\n",
    "        src_vocab_size: int, \n",
    "        tgt_vocab_size: int, \n",
    "        src_seq_len: int, \n",
    "        tgt_seq_len: int,\n",
    "        d_model: int = 512,\n",
    "        N: int = 6,\n",
    "        h: int = 8,\n",
    "        dropout: float = 0.1,\n",
    "        d_ff: int = 2048) -> Transformer:\n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
    "    src_pos = PositionalEncodings(d_model, src_seq_len, dropout)\n",
    "    tgt_pos = PositionalEncodings(d_model, tgt_seq_len, dropout)\n",
    "\n",
    "    encoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        encoder_blocks.append(\n",
    "            EncoderBlock(\n",
    "                MultiHeadAttentionBlock(d_model, h, dropout), \n",
    "                FeedForwardBlock(d_model, d_ff, dropout), \n",
    "                dropout\n",
    "            )\n",
    "        )\n",
    "    encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
    "\n",
    "    decoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        decoder_blocks.append(\n",
    "            DecoderBlock(\n",
    "                MultiHeadAttentionBlock(d_model, h, dropout), \n",
    "                MultiHeadAttentionBlock(d_model, h, dropout), \n",
    "                FeedForwardBlock(d_model, d_ff, dropout), \n",
    "                dropout\n",
    "            )\n",
    "        )\n",
    "    decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
    "\n",
    "    proj_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, proj_layer)\n",
    "\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4928d9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(dataset, lang):\n",
    "    for item in dataset:\n",
    "        yield item[\"translation\"][lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "acedae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_build_tokenizer(config, dataset, lang):\n",
    "    tokenizer_path = Path()(config[\"tokenizer_file\"].format(lang))\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(dataset, lang), trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "01874437",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilingualDataset(nn.Module):\n",
    "    def __init__(self, dataset, tokenizer_src, tokenizer_tgt, lang_src, lang_tgt, seq_len):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.lang_src = lang_src\n",
    "        self.lang_tgt = lang_tgt\n",
    "\n",
    "        self.sos_token = torch.tensor([tokenizer_src.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([tokenizer_src.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([tokenizer_src.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        src = torch.tensor(self.tokenizer_src.encode(item[\"translation\"][self.lang_src]).ids, dtype=torch.int64)\n",
    "        tgt = torch.tensor(self.tokenizer_tgt.encode(item[\"translation\"][self.lang_tgt]).ids, dtype=torch.int64)\n",
    "        \n",
    "        enc_num_pad_tokens = self.seq_len - src.shape[0] - 2\n",
    "        dec_num_pad_tokens = self.seq_len - tgt.shape[0] - 1\n",
    "\n",
    "        if enc_num_pad_tokens < 0 or dec_num_pad_tokens < 0:\n",
    "            raise ValueError(\"Sentence too long\")\n",
    "        \n",
    "        encoder_input = torch.cat([\n",
    "            self.sos_token,\n",
    "            src,\n",
    "            self.eos_token,\n",
    "            self.pad_token.repeat(enc_num_pad_tokens)\n",
    "        ])\n",
    "        decoder_input = torch.cat([\n",
    "            self.sos_token,\n",
    "            tgt,\n",
    "            self.pad_token.repeat(dec_num_pad_tokens)\n",
    "        ])\n",
    "        label = torch.cat([\n",
    "            tgt,\n",
    "            self.eos_token,\n",
    "            self.pad_token.repeat(dec_num_pad_tokens)\n",
    "        ])\n",
    "\n",
    "        assert encoder_input.shape[0] == self.seq_len\n",
    "        assert decoder_input.shape[0] == self.seq_len\n",
    "        assert label.shape[0] == self.seq_len\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input, \n",
    "            \"decoder_input\": decoder_input,\n",
    "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n",
    "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & \n",
    "            (torch.triu(torch.ones(1, self.seq_len, self.seq_len), diagonal=1) == 0).int(),\n",
    "            \"label\": label,\n",
    "            \"src_text\": item[\"translation\"][self.lang_src],\n",
    "            \"tgt_text\": item[\"translation\"][self.lang_tgt]\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8f9cdd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(config):\n",
    "    dataset_raw = load_dataset(\"Helsinki-NLP/opus-100\", f\"{config['lang_src']}-{config['lang_tgt']}\", split=\"train\")\n",
    "\n",
    "    tokenizer_src = get_or_build_tokenizer(config, dataset_raw, config[\"lang_src\"])\n",
    "    tokenizer_tgt = get_or_build_tokenizer(config, dataset_raw, config[\"lang_tgt\"])\n",
    "\n",
    "    train_ds_size = int(0.9 * len(dataset_raw))\n",
    "    val_ds_size = len(dataset_raw) - train_ds_size\n",
    "    train_ds_raw, val_ds_raw = random_split(dataset_raw, [train_ds_size, val_ds_size])\n",
    "\n",
    "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config[\"lang_src\"], config[\"lang_tgt\"], config[\"seq_len\"])\n",
    "    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config[\"lang_src\"], config[\"lang_tgt\"], config[\"seq_len\"])\n",
    "\n",
    "    max_len_src = 0\n",
    "    max_len_tgt = 0\n",
    "    for item in dataset_raw:\n",
    "        max_len_src = max(max_len_src, len(tokenizer_src.encode(item[\"translation\"][config[\"lang_src\"]]).ids))\n",
    "        max_len_tgt = max(max_len_tgt, len(tokenizer_tgt.encode(item[\"translation\"][config[\"lang_tgt\"]]).ids))\n",
    "\n",
    "    print(f\"Max length src: {max_len_src}\")\n",
    "    print(f\"Max length tgt: {max_len_tgt}\")\n",
    "\n",
    "    train_dataloader = DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8fc8c24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
    "    model = build_transformer(vocab_src_len, vocab_tgt_len, config[\"seq_len\"], config[\"seq_len\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "112af76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    return {\n",
    "        \"lang_src\": \"en\",\n",
    "        \"lang_tgt\": \"id\",\n",
    "        \"batch_size\": 8,\n",
    "        \"seq_len\": 600,\n",
    "        \"tokenizer_file\": \"tokenizer_{0}.json\",\n",
    "        \"num_epochs\": 20,\n",
    "        \"lr\": 0.0001,\n",
    "        \"model_folder\": \"weights\",\n",
    "        \"model_basename\": \"tmodel_\",\n",
    "        \"preload\": None,\n",
    "        \"experiment_name\": \"runs/tmodel\",\n",
    "        \"d_model\": 512\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3ab89bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights_file_path(config, epochs):\n",
    "    model_folder = config[\"model_folder\"]\n",
    "    model_basename = config[\"model_basename\"]\n",
    "    model_filename = f\"{model_basename}{epochs}.pt\"\n",
    "    return str(Path(\".\") / model_folder / model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abd7828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
    "    sos_idx = tokenizer_tgt.token_to_id(\"[SOS]\")\n",
    "    eos_idx = tokenizer_tgt.token_to_id(\"[EOS]\")\n",
    "\n",
    "    encoder_output = model.encode(src, src_mask)\n",
    "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(src).to(device)\n",
    "    while True:\n",
    "        if decoder_input.shape[1] >= max_len:\n",
    "            break\n",
    "        decoder_mask = (torch.triu(torch.ones(1, decoder_input.shape[1], decoder_input.shape[1]), diagonal=1) == 0).type_as(src_mask).to(device)\n",
    "        decoder_output = model.decode(encoder_output, src_mask, decoder_input, decoder_mask)\n",
    "        prob = model.project(decoder_output[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        decoder_input = torch.cat([decoder_input, torch.empty(1, 1).type_as(src).fill_(next_word.item()).to(device)], dim=1)\n",
    "\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "\n",
    "    return decoder_input.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcbbebe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation(\n",
    "        model, \n",
    "        val_dataset, \n",
    "        tokenizer_src, \n",
    "        tokenizer_tgt, \n",
    "        max_len, \n",
    "        device, \n",
    "        print_msg, \n",
    "        global_state,\n",
    "        writer,\n",
    "        num_examples = 2):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "\n",
    "    console_width = 80\n",
    "    with torch.no_grad():\n",
    "        for item in val_dataset:\n",
    "            count+=1\n",
    "            encoder_input = item[\"encoder_input\"].to(device)\n",
    "            encoder_mask = item[\"encoder_mask\"].to(device)\n",
    "            \n",
    "            assert encoder_input.shape[0] == 1, \"Batch size must be 1 for validation\"\n",
    "            model_output = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "            \n",
    "            src_text = item[\"src_text\"][0]\n",
    "            expected = item[\"tgt_text\"][0]\n",
    "            predicted = tokenizer_tgt.decode(model_output.detach().cpu().numpy())\n",
    "\n",
    "            print_msg(\"-\" * console_width)\n",
    "            print_msg(f\"Source: {src_text}\")\n",
    "            print_msg(f\"Expected: {expected}\")\n",
    "            print_msg(f\"Predicted: {predicted}\")\n",
    "\n",
    "            if count == num_examples:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dd2cee25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device {device}\")\n",
    "\n",
    "    Path(config[\"model_folder\"]).mkdir(parents=True, exist_ok=True)\n",
    "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_dataset(config)\n",
    "    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"], eps=1e-9)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id(\"[PAD]\"), label_smoothing=0.1).to(device)\n",
    "\n",
    "    writer = SummaryWriter(config[\"experiment_name\"])\n",
    "\n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "    if config[\"preload\"] is not None:\n",
    "        model_filename = get_weights_file_path(config, config[\"preload\"])\n",
    "        print(f\"Preloading model from {model_filename}\")\n",
    "        state = torch.load(model_filename)\n",
    "        initial_epoch = state[\"epoch\"] + 1\n",
    "        optimizer.load_state_dict(state[\"optimizer_state_dict\"])\n",
    "        global_step = state[\"global_step\"]\n",
    "\n",
    "    for epoch in range(initial_epoch, config[\"num_epochs\"]):\n",
    "        batch_iterator = tqdm(train_dataloader, desc=f\"Epoch {epoch}\")\n",
    "        for batch in batch_iterator:\n",
    "            model.train()\n",
    "            encoder_input = batch[\"encoder_input\"].to(device)\n",
    "            decoder_input = batch[\"decoder_input\"].to(device)\n",
    "            encoder_mask = batch[\"encoder_mask\"].to(device)\n",
    "            decoder_mask = batch[\"decoder_mask\"].to(device)\n",
    "\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask)\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
    "            proj_output = model.project(decoder_output)\n",
    "\n",
    "            label = batch[\"label\"].to(device)\n",
    "            loss = criterion(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
    "            batch_iterator.set_postfix({\"loss\": f\"{loss.item(): 6.3f}\"})\n",
    "\n",
    "            writer.add_scalar(\"Loss/train\", loss.item(), global_step)\n",
    "            writer.flush()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        run_validation(\n",
    "            model, \n",
    "            val_dataloader, \n",
    "            tokenizer_src, \n",
    "            tokenizer_tgt, \n",
    "            config[\"seq_len\"], \n",
    "            device, \n",
    "            lambda msg: batch_iterator.write(msg), \n",
    "            global_step, \n",
    "            writer)\n",
    "\n",
    "        model_filename = get_weights_file_path(config, epoch)\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"global_step\": global_step\n",
    "        }, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713c224d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config()\n",
    "train_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03250ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config()\n",
    "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_dataset(config)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "model_filename = get_weights_file_path(config, 19)\n",
    "state = torch.load(model_filename)\n",
    "model.load_state_dict(state[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "002a5017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import decoder\n",
    "\n",
    "\n",
    "def load_nex_batch():\n",
    "    batch = next(iter(val_dataloader))\n",
    "    encoder_input = batch[\"encoder_input\"].to(device)\n",
    "    encoder_mask = batch[\"encoder_mask\"].to(device)\n",
    "    decoder_input = batch[\"decoder_input\"].to(device)\n",
    "    decoder_mask = batch[\"decoder_mask\"].to(device)\n",
    "\n",
    "    encoder_input_token = [tokenizer_src.id_to_token(x) for x in encoder_input[0].cpu().numpy()]\n",
    "    decoder_input_token = [tokenizer_tgt.id_to_token(x) for x in decoder_input[0].cpu().numpy()]\n",
    "\n",
    "    model_output = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, config[\"seq_len\"], device)\n",
    "\n",
    "    return batch, encoder_input_token, decoder_input_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0c5d6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mtx2df(m, max_row, max_col, row_tokens, col_tokens):\n",
    "    return pd.DataFrame(\n",
    "        [\n",
    "            (\n",
    "                r,\n",
    "                c,\n",
    "                float(m[r, c]),\n",
    "                \"%.3d %s\" % (r, row_tokens[r] if len(row_tokens) > r else \"<blank>\"),\n",
    "                \"%.3d %s\" % (c, col_tokens[c] if len(col_tokens) > c else \"<blank>\"),\n",
    "            )\n",
    "            for r in range(m.shape[0])\n",
    "            for c in range(m.shape[1])\n",
    "            if r < max_row and c < max_col\n",
    "        ],\n",
    "        columns=[\"row\", \"column\", \"value\", \"row_token\", \"col_token\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a651f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_map(attn_type: str, layer: int, head: int):\n",
    "    if attn_type == \"encoder\":\n",
    "        attn = model.encoder.layers[layer].self_attention_block.attention_scores\n",
    "    elif attn_type == \"decoder\":\n",
    "        attn = model.decoder.layers[layer].self_attention_block.attention_scores\n",
    "    elif attn_type == \"encoder-decoder\":\n",
    "        attn = model.decoder.layers[layer].cross_attention_block.attention_scores\n",
    "    return attn[0, head].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a17d1e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attn_map(attn_type, layer, head, row_tokens, col_tokens, max_sentence_len):\n",
    "    df = mtx2df(\n",
    "        get_attn_map(attn_type, layer, head),\n",
    "        max_sentence_len,\n",
    "        max_sentence_len,\n",
    "        row_tokens,\n",
    "        col_tokens,\n",
    "    )\n",
    "    return (\n",
    "        alt.Chart(data=df)\n",
    "        .mark_rect()\n",
    "        .encode(\n",
    "            x=alt.X(\"col_token\", axis=alt.Axis(title=\"\")),\n",
    "            y=alt.Y(\"row_token\", axis=alt.Axis(title=\"\")),\n",
    "            color=\"value\",\n",
    "            tooltip=[\"row\", \"column\", \"value\", \"row_token\", \"col_token\"],\n",
    "        )\n",
    "        #.title(f\"Layer {layer} Head {head}\")\n",
    "        .properties(height=400, width=400, title=f\"Layer {layer} Head {head}\")\n",
    "        .interactive()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93e567df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_attention_maps(attn_type: str, layers: list[int], heads: list[int], row_tokens: list, col_tokens, max_sentence_len: int):\n",
    "    charts = []\n",
    "    for layer in layers:\n",
    "        rowCharts = []\n",
    "        for head in heads:\n",
    "            rowCharts.append(attn_map(attn_type, layer, head, row_tokens, col_tokens, max_sentence_len))\n",
    "        charts.append(alt.hconcat(*rowCharts))\n",
    "    return alt.vconcat(*charts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4892ff3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, encoder_input_token, decoder_input_token = load_nex_batch()\n",
    "print(f\"Source: {batch['src_text'][0]}\")\n",
    "print(f\"Expected: {batch['tgt_text'][0]}\")\n",
    "sentence_len = encoder_input_token.index(\"[PAD]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eee5d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [0, 1, 2]\n",
    "heads = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "get_all_attention_maps(\"encoder\", layers, heads, encoder_input_token, encoder_input_token, min(20, sentence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b2f189",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_attention_maps(\"decoder\", layers, heads, decoder_input_token, decoder_input_token, min(20, sentence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269f238f",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_attention_maps(\"encoder-decoder\", layers, heads, encoder_input_token, decoder_input_token, min(20, sentence_len))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
